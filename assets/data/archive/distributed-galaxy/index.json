{"hash":"c32ecfdb33b075b0057598b5b9118f3554a398cf","data":{"article":{"id":"9fb3ee5cbba9adb9f306c5529028fa58","title":"","tease":"","image":"","images":{},"category":null,"contact":"","date":null,"content":"<p>Saving small snippets of the discussion for future reference:</p>\n<hr>\n<p>Istvan:</p>\n<p>I think the most important thing that we need to start thinking\nabout is the how to create a distributed computational system. Think\nabout a thousand users all wanting to start 10 queries. How do we do\nit? How could we implement a demonstration prototype of  this in say\none month? It is doable if we keep things simple.</p>\n<hr>\n<p>Rico: </p>\n<p>take a look at <a href=\"http://www.bx.psu.edu/~rico/galaxy\" target=\"_blank\" rel=\"noopener noreferrer\">http://www.bx.psu.edu/~rico/galaxy</a></p>\n<hr>\n<p>James: </p>\n<p>But what about if the front end is running on multiple nodes? Sharing\na BDB file over NFS is a <em>bad idea</em> for this case. A database that\nall the front ends can talk to gives more flexibility. It can\nimplement table/row level locking and other nice things to allow\nmultiple processes to keep their data synchronized easily.</p>\n<p>I think we need to strike the right balance. Perhaps every query the\nuser is working on doesn't need to go to the database, they can be\nstored in the users current session, but user accounts, saved\nhistories, and in general things that persist across multiple user\nsessions should be stored in a way that can be easily shared across\nmany systems.</p>\n<p>Also, a SQL database makes analysis of the data easy. When you want\nto aggregate what sorts of queries users are running, how long the\ntake, et cetera it is very useful -- and we will want to do these\nthings.</p>\n<p>At some point we're going to run up against the dreaded GIL, or some other limit with one server and one\npython interpreter. We should plan to easily accomodate running N web\nservers with M interpreters on each behind a session-affinity\npreserving load balancer. Similarly, we may want to look at a less\nthread heavy job queueing strategy so that expensive compute jobs can\nbe offloaded to other systems.</p>\n<hr>\n<p>Istvan:</p>\n<p>What both of you are proposing relies on sophisticated servers that\nare needed to take care of buisness which is a very solid strategy and\nsome things can only be done that way.</p>\n<p>But there is this other approach where every client is a lot more\nindependent, like bittorrent, or to a lesser extent seti@home,\narchitectures where simple systems can come together and become\nsubstantially more efficient.</p>\n<p>Where you could just start a \"trusted\" galaxy server anywhere in the\nworld, they would check in with a main server  so that it will be\nforwarded new users,  while in the background synchronization\nprocesses would start transferring the new data that gets entered into\nthem to a main repository. If this new system needs to be taken\noffline it will notify the main server and would exit once all of  its\ndata is transferred all the while the main server would uppload the\ndata to someone else's system so that when the user comes back they\ndon't notice anything.</p>\n<p>This is one of the reasons I'd like to see galaxy entirely self\ncontained and independent.</p>\n<hr>\n<p>Rico :</p>\n<p>The method used to store all of the \"real\" data (alignments,\nsequences, annotation, etc.  what else?) can for the most part be\nfactored out of the discussion regarding the Web and Compute servers.</p>\n<p>One solution is to separate the storage from the Web and Compute\nservers.  In order to maintain scalability, and availability we could\ngo with something like Luster:  <a href=\"http://www.clusterfs.com\" target=\"_blank\" rel=\"noopener noreferrer\">http://www.clusterfs.com</a>.\nEssentially, there would be another cluster dedicated to providing\naccess to the \"real\" data to the Web and Compute servers.  With this\narchitecture, and proper planning, the failure of a machine in the\ninfrastructure is non-fatal.  A failed Web, Compute, or Storage\nserver is simply replaced, or removed from service.</p>\n<p>Another solution would be to use PVFS: <a href=\"http://www.pvfs.org/pvfs2\" target=\"_blank\" rel=\"noopener noreferrer\">http://www.pvfs.org/pvfs2</a>.\nThis solution would distribute the data across all of the Nodes in\nthe Compute Cluster.  I'd have to look at pfvs again to intelligently\ncomment on it's fault-tolerance capabilities.  We wouldn't want a\nfailed Compute Node to prevent us from accessing some of the \"real\"\ndata.</p>\n"}},"context":{}}