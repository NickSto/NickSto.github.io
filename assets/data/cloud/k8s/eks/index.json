{"hash":"4f66c62fb6940f4cd6ecf5e4a1e2d62361ec5ff4","data":{"article":{"id":"b058438c68f0330efd585a26ddc2ed9f","title":"Deploy an Instance of Galaxy on Amazon Elastic Kubernetes Service","tease":"","category":null,"date":null,"days":null,"contact":"","contact_url":"","fileInfo":{"path":"build/content-md/cloud/k8s/eks/index.md"},"authors":"","location":"","location_url":"","source_blog":"","source_blog_url":"","skip_title_render":null,"redirect":"","autotoc":null,"links":[],"image":"","images":{},"external_url":"","content":"<div class=\"toc-wrapper col-md-3\">\n<ul>\n<li><a href=\"#create-a-cluster\">Create a Cluster</a></li>\n<li><a href=\"#install-helm\">Install Helm</a></li>\n<li><a href=\"#deploy-galaxy-on-the-cluster\">Deploy Galaxy on the Cluster</a></li>\n<li><a href=\"#access-galaxy-instance\">Access Galaxy Instance</a></li>\n<li><a href=\"#delete-eks-cluster\">Delete EKS Cluster</a></li>\n</ul>\n</div>\n<div class=\"body-wrapper col-md-9\">\n<p>We break the steps of deploying Galaxy on\n<a href=\"https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Elastic Kubernetes Service (Amazon EKS)</a>\ninto the following sections:</p>\n<ul>\n<li><a href=\"#create-a-cluster\">Create a k8s cluster</a>;</li>\n<li><a href=\"#install-helm\">Install Helm</a>;</li>\n<li><a href=\"#deploy-galaxy-on-the-cluster\">Deploy an instance of Galaxy using Helm charts</a>;</li>\n<li><a href=\"#access-galaxy-instance\">Access the deployed Galaxy instance</a>;</li>\n<li><a href=\"#delete-eks-cluster\">Delete cluster</a>.</li>\n</ul>\n<h1 id=\"create-a-cluster\"><a href=\"#create-a-cluster\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Create a Cluster</h1>\n<p>Amazon EKS offers two methods for creating a cluster: either via console,\nor by using a tool named <code>eksctl</code>.</p>\n<p>When creating a cluster, you will need to make the following\ndecisions, taking note of the choices you make:</p>\n<ol>\n<li>\n<p>Choose a <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html\" target=\"_blank\" rel=\"noopener noreferrer\">region</a>:\nthe region selected must have Amazon Secure Token Service (STS)<br>\nactivated for your account. You do not necessarily have STS activate for all\nregions under your account; please consult with your account admin. If you\nchoose a region where STS is not activated for your account, you may get an\nerror message as the following:</p>\n<pre><code class=\"language-bash\">AWS::EKS::Cluster/ControlPlane: CREATE_FAILED – \nSTS is not activated in this region for account:0123456789. \nYour account administrator can activate STS in this region using the IAM Console. \n</code></pre>\n</li>\n<li>Choose an <a href=\"https://aws.amazon.com/ec2/instance-types/\" target=\"_blank\" rel=\"noopener noreferrer\">instance type</a>;\ntwo points to consider: first, not necessarily all instance types are available in\nall regions; therefore, choose an instance type that is available in the\nzone where you have STS activated. Second, choose an instance with a \"reasonable\"\namount of resources, as you will be running multiple pods within that instance,\nand if you do not have enough resources, your pods will fail to schedule. For\ninstance, avoid instance types such as <code>a1.medium</code>.</li>\n<li>Cluster node count. While you can deploy Galaxy on a k8s cluster with\nmultiple nodes, in the following we discuss how to deploy on a cluster with\na single node, as there is an additional step for deployment on a multi-node\ncluster that will be discussed separately.</li>\n</ol>\n<p>To create a cluster on on EKS, you may follow these tutorials:</p>\n<ul>\n<li>via console; read <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html\" target=\"_blank\" rel=\"noopener noreferrer\">how to start a cluster using the console</a>;</li>\n<li>via the <code>eksctl</code> CLI tool; read <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html\" target=\"_blank\" rel=\"noopener noreferrer\">how to start a cluster using eksctl</a></li>\n</ul>\n<p>If you have used the <code>eksctl</code> tool to start a cluster, you may have run a\ncommand similar to the following, which creates a cluster with <code>1</code> node of\n<code>m5.4xlarge</code> instance type in <code>us-east-1</code> zone:</p>\n<pre><code class=\"language-bash\">$ eksctl create cluster --name CLUSTER-NAME --version 1.13 --nodegroup-name standard-workers --node-type m5.4xlarge --nodes 1 --nodes-min 1 --nodes-max 1 --node-ami auto --region us-east-1\n</code></pre>\n<p>Note that the instance type and zone values are arbitrary and you may choose any\nvalues following the afore-discussed points.</p>\n<p>If the command execution is successful, you may see an output as:</p>\n<pre><code class=\"language-bash\">[ℹ]  using region us-east-1\n[ℹ]  setting availability zones to [us-east-1b us-east-1c]\n[ℹ]  subnets for us-east-1b - public:192.168.0.0/19 private:192.168.64.0/19\n[ℹ]  subnets for us-east-1c - public:192.168.32.0/19 private:192.168.96.0/19\n[ℹ]  nodegroup \"standard-workers\" will use \"ami-...\" [AmazonLinux2/1.13]\n[ℹ]  using Kubernetes version 1.13\n[ℹ]  creating EKS cluster \"CLUSTER-NAME\" in \"us-east-1\" region\n[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup\n[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --name=CLUSTER-NAME'\n[ℹ]  2 sequential tasks: { create cluster control plane \"CLUSTER-NAME\", create nodegroup \"standard-workers\" }\n[ℹ]  building cluster stack \"eksctl-CLUSTER-NAME-cluster\"\n[ℹ]  deploying stack \"eksctl-CLUSTER-NAME-cluster\"\n[ℹ]  building nodegroup stack \"eksctl-CLUSTER-NAME-nodegroup-standard-workers\"\n[ℹ]  deploying stack \"eksctl-CLUSTER-NAME-nodegroup-standard-workers\"\n[✔]  all EKS cluster resource for \"CLUSTER-NAME\" had been created\n[✔]  saved kubeconfig as \"/Users/CLUSTER-NAME/.kube/config\"\n[ℹ]  adding role \"arn:aws:iam::...:role/eksctl-CLUSTER-NAME-nodegroup-standard-w-NodeInstanceRole-...\" to auth ConfigMap\n[ℹ]  nodegroup \"standard-workers\" has 0 node(s)\n[ℹ]  waiting for at least 1 node(s) to become ready in \"standard-workers\"\n[ℹ]  nodegroup \"standard-workers\" has 1 node(s)\n[ℹ]  node \"ip-192-168-47-193.ec2.internal\" is ready\n[ℹ]  kubectl command should work with \"/Users/CLUSTER-NAME/.kube/config\", try 'kubectl get nodes'\n[✔]  EKS cluster \"CLUSTER-NAME\" in \"us-east-1\" region is ready\n</code></pre>\n<h1 id=\"install-helm\"><a href=\"#install-helm\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Install Helm</h1>\n<p>In order to install and configure Helm on an EKS cluster, you may use\n<a href=\"https://docs.aws.amazon.com/eks/latest/userguide/helm.html\" target=\"_blank\" rel=\"noopener noreferrer\">this documentation</a>.</p>\n<h1 id=\"deploy-galaxy-on-the-cluster\"><a href=\"#deploy-galaxy-on-the-cluster\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Deploy Galaxy on the Cluster</h1>\n<p>Once you have established that you have Helm and have configured the <code>tiller</code>\nserver (as detailed in the above documentation), the deployment of a Galaxy on\na k8s cluster consists of two steps:</p>\n<ol>\n<li>\n<p>[Optional] deploy the <a href=\"https://cernvm.cern.ch/portal/filesystem\" target=\"_blank\" rel=\"noopener noreferrer\">CernVM File System (CVMFS)</a> chart.\nYou may run the following commands for this deployment:</p>\n<pre><code class=\"language-bash\">$ git clone https://github.com/CloudVE/galaxy-cvmfs-csi-chart.git\n$ cd galaxy-cvmfs-csi-chart/galaxy-cvmfs-csi\n$ helm install .\n</code></pre>\n<p>Output may look like the following:</p>\n<pre><code class=\"language-bash\">NAME:   wistful-sasquatch\nLAST DEPLOYED: Thu Aug  8 11:13:35 2019\nNAMESPACE: default\nSTATUS: DEPLOYED\n\nRESOURCES:\n==> v1/ClusterRole\nNAME                            AGE\ncvmfs-external-attacher-runner  3s\ncvmfs-nodeplugin                3s\ncvmfs-provisioner-runner        3s\n\n==> v1/ClusterRoleBinding\nNAME                    AGE\ncvmfs-attacher-role     3s\ncvmfs-nodeplugin        3s\ncvmfs-provisioner-role  3s\n\n==> v1/ConfigMap\nNAME             DATA  AGE\ncvmfs-configmap  3     4s\n\n==> v1/PersistentVolume\nNAME            CAPACITY  ACCESS MODES  RECLAIM POLICY  STATUS  CLAIM                    STORAGECLASS  REASON  AGE\ncvmfs-cache-pv  1000Mi    RWX           Retain          Bound   default/cvmfs-cache-pvc  manual        4s\n\n==> v1/PersistentVolumeClaim\nNAME             STATUS  VOLUME          CAPACITY  ACCESS MODES  STORAGECLASS  AGE\ncvmfs-cache-pvc  Bound   cvmfs-cache-pv  1000Mi    RWX           manual        4s\n\n==> v1/Pod(related)\nNAME                           READY  STATUS             RESTARTS  AGE\ncsi-cvmfsplugin-9gl65          0/2    ContainerCreating  0         3s\ncsi-cvmfsplugin-attacher-0     0/1    ContainerCreating  0         3s\ncsi-cvmfsplugin-provisioner-0  0/1    ContainerCreating  0         2s\n\n==> v1/Service\nNAME                         TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)    AGE\ncsi-cvmfsplugin-attacher     ClusterIP  10.100.151.23  &#x3C;none>       12345/TCP  3s\ncsi-cvmfsplugin-provisioner  ClusterIP  10.100.64.145  &#x3C;none>       12345/TCP  3s\n\n==> v1/ServiceAccount\nNAME               SECRETS  AGE\ncvmfs-attacher     1        3s\ncvmfs-nodeplugin   1        3s\ncvmfs-provisioner  1        3s\n\n==> v1/StorageClass\nNAME            PROVISIONER      AGE\ncvmfs-gxy-data  csi-cvmfsplugin  4s\ncvmfs-gxy-main  csi-cvmfsplugin  4s\n\n==> v1beta1/StatefulSet\nNAME                         READY  AGE\ncsi-cvmfsplugin-attacher     0/1    3s\ncsi-cvmfsplugin-provisioner  0/1    2s\n\n==> v1beta2/DaemonSet\nNAME             DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE\ncsi-cvmfsplugin  1        1        0      1           0          &#x3C;none>         3s\n</code></pre>\n</li>\n<li>\n<p>Deploy Galaxy chart, using the following commands:</p>\n<pre><code class=\"language-bash\">$ git clone https://github.com/galaxyproject/galaxy-helm.git\n$ cd galaxy-helm/galaxy/\n$ helm dependency update\n$ helm install . --set persistence.accessMode=ReadWriteOnce --set service.type=LoadBalancer --set service.port=80 -f values-cvmfs.yaml\n</code></pre>\n<p>The output should look like the following:</p>\n<pre><code class=\"language-bash\">NAME:   wistful-quoll\nLAST DEPLOYED: Thu Aug  8 11:21:33 2019\nNAMESPACE: default\nSTATUS: DEPLOYED\n\nRESOURCES:\n==> v1/ConfigMap\nNAME                            DATA  AGE\nwistful-quoll-galaxy-configs    8     2s\nwistful-quoll-galaxy-job-rules  2     3s\n\n==> v1/Deployment\nNAME                      READY  UP-TO-DATE  AVAILABLE  AGE\nwistful-quoll-galaxy-job  0/1    1           0          2s\nwistful-quoll-galaxy-web  0/1    1           0          2s\n\n==> v1/PersistentVolumeClaim\nNAME                                     STATUS   VOLUME          CAPACITY  ACCESS MODES  STORAGECLASS  AGE\nwistful-quoll-galaxy-cvmfs-gxy-data-pvc  Pending  cvmfs-gxy-data  2s\nwistful-quoll-galaxy-cvmfs-gxy-main-pvc  Pending  cvmfs-gxy-main  2s\nwistful-quoll-galaxy-pvc                 Pending  gp2             2s\n\n==> v1/Pod(related)\nNAME                                       READY  STATUS   RESTARTS  AGE\nwistful-quoll-galaxy-job-5c4f897669-k46vk  0/1    Pending  0         2s\nwistful-quoll-galaxy-postgres-0            0/1    Pending  0         2s\nwistful-quoll-galaxy-web-595f59c5b-gx974   0/1    Pending  0         2s\n\n==> v1/Role\nNAME                                AGE\nwistful-quoll-galaxy-role-pod-jobs  2s\n\n==> v1/RoleBinding\nNAME                            AGE\nwistful-quoll-galaxy-batch-ops  2s\n\n==> v1/Secret\nNAME                           TYPE    DATA  AGE\nwistful-quoll-galaxy-initdb    Opaque  2     3s\nwistful-quoll-galaxy-postgres  Opaque  1     3s\n\n==> v1/Service\nNAME                                    TYPE          CLUSTER-IP      EXTERNAL-IP  PORT(S)       AGE\nwistful-quoll-galaxy                    LoadBalancer  10.100.54.20    &#x3C;pending>    80:30691/TCP  2s\nwistful-quoll-galaxy-postgres           ClusterIP     10.100.169.181  &#x3C;none>       5432/TCP      2s\nwistful-quoll-galaxy-postgres-headless  ClusterIP     None            &#x3C;none>       5432/TCP      2s\n\n==> v1beta1/Ingress\nNAME                  HOSTS  ADDRESS  PORTS  AGE\nwistful-quoll-galaxy  *      80       2s\n\n==> v1beta2/StatefulSet\nNAME                           READY  AGE\nwistful-quoll-galaxy-postgres  0/1    2s\n</code></pre>\n</li>\n</ol>\n<h1 id=\"access-galaxy-instance\"><a href=\"#access-galaxy-instance\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Access Galaxy Instance</h1>\n<p>Having deployed Galaxy on an EKS cluster, you may access the instance as the following:</p>\n<ol>\n<li>\n<p>Run <code>kubectl get svc</code>, which lists the services as the following:</p>\n<pre><code class=\"language-bash\">NAME                                     TYPE           CLUSTER-IP       EXTERNAL-IP        PORT(S)        AGE\ncsi-cvmfsplugin-attacher                 ClusterIP      10.100.151.23    &#x3C;none>             12345/TCP      17m\ncsi-cvmfsplugin-provisioner              ClusterIP      10.100.64.145    &#x3C;none>             12345/TCP      17m\nkubernetes                               ClusterIP      10.100.0.1       &#x3C;none>             443/TCP        47m\nwistful-quoll-galaxy                     LoadBalancer   10.100.54.20     a5d718acbba09...   80:30691/TCP   9m\nwistful-quoll-galaxy-postgres            ClusterIP      10.100.169.181   &#x3C;none>             5432/TCP       9m\nwistful-quoll-galaxy-postgres-headless   ClusterIP      None             &#x3C;none>             5432/TCP       9m\n</code></pre>\n</li>\n<li>\n<p>Copy the <code>Name</code> of the <code>LoadBalancer</code> service (in this case,\n'wistful-quoll-galaxy'), and run the following command:</p>\n<pre><code class=\"language-bash\">$ kubectl describe svc wistful-quoll-galaxy\n</code></pre>\n<p>where <code>wistful-quoll-galaxy</code> is the name of the <code>LoadBalancer</code> in our example, that\nshould be replaced by the name of <code>LoadBalancer</code> in your deployment.</p>\n<p>The output of this command is as the following:</p>\n<pre><code class=\"language-bash\">Name:                     wistful-quoll-galaxy\nNamespace:                default\nLabels:                   app.kubernetes.io/instance=wistful-quoll\n                          app.kubernetes.io/managed-by=Tiller\n                          app.kubernetes.io/name=galaxy\n                          helm.sh/chart=galaxy-3.0.0\nAnnotations:              &#x3C;none>\nSelector:                 app.kubernetes.io/instance=wistful-quoll,app.kubernetes.io/name=wistful-quoll-galaxy,component=galaxy-web-handler\nType:                     LoadBalancer\nIP:                       10.100.54.20\nLoadBalancer Ingress:     a0a000aaaaa0000a000aa0a0a00a00aa-000000000.us-east-1.elb.amazonaws.com\nPort:                     http  80/TCP\nTargetPort:               galaxy-http/TCP\nNodePort:                 http  30691/TCP\nEndpoints:                192.168.32.240:8080\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:\n  Type    Reason                Age   From                Message\n  ----    ------                ----  ----                -------\n  Normal  EnsuringLoadBalancer  10m   service-controller  Ensuring load balancer\n  Normal  EnsuredLoadBalancer   10m   service-controller  Ensured load balancer\n</code></pre>\n<p>The public IP address from which the Galaxy instance is available is given in the\n<code>LoadBalancer Ingress</code> field.</p>\n</li>\n</ol>\n<h1 id=\"delete-eks-cluster\"><a href=\"#delete-eks-cluster\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Delete EKS Cluster</h1>\n<p>To delete all resources associated with the cluster (such as nodes, the load\nbalancer, and volumes) and avoid orphaned resources, a few steps must be taken:</p>\n<ol>\n<li>\n<p>Run <code>kubectl get svc --all-namespaces</code>, whose output may look like the following:</p>\n<pre><code class=\"language-bash\">NAMESPACE     NAME                                     TYPE           CLUSTER-IP       EXTERNAL-IP        PORT(S)         AGE\ndefault       csi-cvmfsplugin-attacher                 ClusterIP      10.100.151.23    &#x3C;none>             12345/TCP       26m\ndefault       csi-cvmfsplugin-provisioner              ClusterIP      10.100.64.145    &#x3C;none>             12345/TCP       26m\ndefault       kubernetes                               ClusterIP      10.100.0.1       &#x3C;none>             443/TCP         56m\ndefault       wistful-quoll-galaxy                     LoadBalancer   10.100.54.20     a5d718acbba09...   80:30691/TCP    18m\ndefault       wistful-quoll-galaxy-postgres            ClusterIP      10.100.169.181   &#x3C;none>             5432/TCP        18m\ndefault       wistful-quoll-galaxy-postgres-headless   ClusterIP      None             &#x3C;none>             5432/TCP        18m\nkube-system   kube-dns                                 ClusterIP      10.100.0.10      &#x3C;none>             53/UDP,53/TCP   56m\n</code></pre>\n</li>\n<li>\n<p>Copy the <code>NAME</code> of the <code>LoadBalancer</code> service, and run the following command:</p>\n<pre><code class=\"language-bash\">$ kubectl delete svc wistful-quoll-galaxy\n</code></pre>\n<p>where <code>wistful-quoll-galaxy</code> is the <code>NAME</code> of the <code>LoadBalancer</code> in our\nexample. This will prevent normal failure recovery procedures from\nrecreating any terminated instances.</p>\n</li>\n<li>\n<p>To delete the cluster, run the following command replacing <code>CLUSTER-NAME</code> and <code>REGION</code>\nwith the name of your cluster and region, respectively, that you assigned when\ncreating the cluster.</p>\n<pre><code class=\"language-bash\">$ eksctl delete cluster --name CLUSTER-NAME --region REGION\n</code></pre>\n<p>Output:</p>\n<pre><code class=\"language-bash\">[ℹ]  using region us-east-1\n[ℹ]  deleting EKS cluster \"CLUSTER-NAME\"\n[✔]  kubeconfig has been updated\n[ℹ]  cleaning up LoadBalancer services\n[ℹ]  2 sequential tasks: { delete nodegroup \"standard-workers\", delete cluster control plane \"CLUSTER-NAME\" [async] }\n[ℹ]  will delete stack \"eksctl-CLUSTER-NAME-nodegroup-standard-workers\"\n[ℹ]  waiting for stack \"eksctl-CLUSTER-NAME-nodegroup-standard-workers\" to get deleted\n[ℹ]  will delete stack \"eksctl-CLUSTER-NAME-cluster\"\n[✔]  all cluster resources were deleted\n</code></pre>\n</li>\n<li>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-ebs-volume.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Elastic Block Store (EBS)</a>\nvolumes may not delete automatically. To delete them manually, take the following steps:</p>\n<ul>\n<li>Goto <a href=\"https://console.aws.amazon.com/ec2/\" target=\"_blank\" rel=\"noopener noreferrer\">https://console.aws.amazon.com/ec2/</a>;</li>\n<li>Choose <code>Volumes</code> under the <code>ELASTIC BLOCK STORE</code> category;</li>\n<li>choose the related volumes for the list;</li>\n<li>click on the <code>Actions</code> button, then the <code>Delete Volumes</code> menu item.</li>\n</ul>\n</li>\n</ol>\n</div>\n"}},"context":{}}