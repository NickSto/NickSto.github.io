{"hash":"a97cc003ab7b7b0128b2927bf289a961e571a0b8","data":{"article":{"id":"8411af4da73d6c636e1001c375dcbb85","title":"In the Future","tease":"","category":"","date":null,"days":null,"contact":"","contact_url":"","authors":"","location":"","location_url":"","source_blog":"","source_blog_url":"","skip_title_render":null,"redirect":"","autotoc":null,"links":[],"image":"","images":{},"inserts":[{"name":"/events/gcc2013/header","content":"<div class=\"text-center\">\n[<img class=\"img-fluid mx-auto\" src=\"/images/logos/GCC2013Logo400.png\" alt=\"2013 Galaxy Community Conference (GCC2013), Oslo, Norway, 30 June - 2 July, 2013\" />](/src/events/gcc2013/index.md)\n<br />\n**[Watch the talks](http://vimeo.com/channels/581875)!  [See you at GCC2018](https://gccbosc2018.sched.com/)!**\n<br /><br />\n</div>\n"},{"name":"/events/gcc2013/linkbox","content":"<div class=\"alert alert-info float-right text-center trim-p\">\n<p><a href=\"/events/gcc2013/\"><strong>GCC2013</strong></a><br>\n<a href=\"/events/gcc2013/program/\">Program</a><br>\n<a href=\"/events/gcc2013/training-day/\">Training</a><br>\n<a href=\"/events/gcc2013/bof/\">BoFs</a><br>\n<a href=\"/events/gcc2013/abstracts/\">Abstracts</a><br>\n<a href=\"/events/gcc2013/logistics/\">Logistics</a><br>\n<a href=\"/events/gcc2013/sponsorships/\">Sponsors</a><br>\n<a href=\"/events/gcc2013/key-dates/\">Key Dates</a><br>\n<a href=\"/events/gcc2013/register/\">Register</a><br>\n<a href=\"/events/gcc2013/promotion/\">Promotion</a><br>\n<a href=\"/events/gcc2013/organizers/\">Organizers</a></p>\n</div>\n"},{"name":"/events/gcc2013/footer","content":"<p><br /><br /></p>\n<hr>\n<p>Questions? Contact the <a href=\"/events/gcc2013/organizers/\">Organizers</a>.</p>\n"}],"external_url":"","content":"\ntable-of-contents\n=================\n\nend-table-of-contents\n=====================\n\n<slot name=\"/events/gcc2013/header\" />\n\n<slot name=\"/events/gcc2013/linkbox\" />\n\n<div class='right'><a href=\"/events/gcc2013/training-day/\"><img src=\"/images/logos/GCC2013TrainingDayLogo300.png\" alt=\"Training Day\" width=\"200\" /></a></div>\n\nThe Galaxy API\n==============\n\nThe Galaxy API is a way of interacting with Galaxy data without using the web-based, user interface.\n\nA more thorough explanation can be found at [Learn/API](/learn/api/) along with some general steps on\nhow to begin using the API. This page may duplicate some of that data in order to keep information relevant to the\nworkshop in one place.\n\nFull documentation for each API function and module is posted at\n[ReadTheDocs.org](http://galaxy-dist.readthedocs.org/en/latest/lib/galaxy.webapps.galaxy.api.html).\n\nThis Wiki page is the outline of a 2013 GCC workshop that attempts to teach the conventions of Galaxy's API\nwhile building an example script.\n\n<br />\n</div></div>\n\nWhat's an API and why would I care?\n-----------------------------------\n\nAn API (Application Programming Interface) is the syntax defined by a program for how it can be controlled by another.\n\nA web (REST or REpresentational State Transfer) API is just an another API where the syntax,\ninstead of function calls and their arguments, is made up of a series of URLs and HTTP requests. The server providing\nthe web API becomes the program and the URLs become the commands - all through the *medium* of the HTTP protocol.\n\nImagine if your home had a web API. You might be able to turn on the lights simply by typing\n`myhouse.net/lights/dining-room/on`, `myhouse.net/fireplace/on?setting=low`, etc. Each URL entered\nwould tell this external system (your home) to perform a single action.\n\nYou certainly can do these things yourself by manually turning light switches on and lighting fires -\nbut the key advantages to an API are that you can:\n\n1. Iteratively accomplish tasks:\n   `for every book in the bedroom, return the book to the bookshelf (ok - that probably won't be possible any time soon -\n     but wouldn't it be nice)`\n2. Scheduled or responsive tasks even when you're not around:\n   `when I'm on vacation, feed the cat everyday at 8am and bring in the mail at 2pm`\n3. Compose complex tasks/scripts from simple tasks to remove the tedium of common scenarios:\n   `wake me up at 6:00am, turn on the lights, and start making some frighteningly strong coffee` â†’ `ohgodmorning.py`\n\nIn more Galaxy specific terms, the web API allows you to use Galaxy's capabilities programmatically:\nrunning a workflow *over each file* in a directory, moving a dataset to a library *only if* it passes some QA\nthreshold, upload a fastq file *when* the sequencer finishes writing it, or *combine* any or all of the above into\nanother script.\n\nAdditionally, and perhaps more importantly: even when your scripts run, the features that make Galaxy great are still\napplied to your data:\n\n* histories still capture the exact steps of your experiments and reproducibility is maintained,\n* jobs and compute resources are still managed for you,\n* datasets and metadata persist and are centrallized,\n* your work is still sharable,\n* etc.\n\nIt's also worth noting that all the work you did via the API is still accessible and modifiable when you return to the UI.\n\n<br />\n-----------------------------------------------------------\n\nHow to Access and Use the API\n=============================\n\nAnything that can communicate over HTTP can use the Galaxy web API.\n\n* browser - capable of only simple GET API methods\n* wget - only GET methods - but from the command line\n* curl - a unix program that will let you use any API method\n\nSome programming or scripting languages have their own libraries to do this:\n\n* python - urllib, urllib2 (can be complicated), requests (simplifies, but still many options)\n* javascript - jquery (use any API method, browser only), node (any method, command line)\n* bash - curl + bash = you're probably a bioinformaticist\n\nThere are several scripts and excellent programming libraries designed to help with the Galaxy API:\n\n* scripts/api - a small selection of simple scripts and examples that can help you write and explore interacting with\n  the galaxy API in python, located in the root directory of a Galaxy installation\n* bioblend - ([source](https://github.com/afgane/bioblend), [documentation](http://blend.readthedocs.org/en/latest/))\n  a library for use in python that greatly simplifies writing complex Galaxy API interactions\n* blend4j - ([source](https://github.com/jmchilton/blend4j), [documentation](https://github.com/jmchilton/blend4j#readme))\n  a library for use in Java based on bioblend\n\nData returned from (and in some cases passed to) the Galaxy API is often described or formatted in\n[JSON](http://en.wikipedia.org/wiki/JSON) (JavaScript Object Notation) - a compact and flexible way of describing\nrelatively complex data structures. Conversion to the language you may be programming in is automatic in the case of the\nscripts/api directory, BioBlend, and !Blend4j and relatively easy for python, and a non-issue for JavaScript.\n\n<br />\n\nThe concepts of resources, verbs, and the REST model\n----------------------------------------------------\n\nA common way to construct an API command is to view it as a simple (imperative) sentence: \\[verb]  \\[a resource]\n\n* What are resources? The datasets you upload and generate from tools, the tools themselves, jobs, histories,\n  libraries, even you (a `galaxy_user`) - essentially any *thing* in Galaxy that is recorded in the database.\n* Your verbs are [CRUD (Create, Retrieve, Update, Delete)](http://en.wikipedia.org/wiki/CRUD) - considered by many to\n  be the four building blocks of any interaction with a database and, because of that, resources. Some map to the\n  sentence metaphor well (e.g. `delete a history`) others do not (e.g. `create a workflow` would *run* a workflow).\n* The HTTP versions of CRUD are POST (Create), GET (Retrieve), PUT (Update), and DELETE (Delete). There are\n  others but they don't apply (yet). These are\n  [HTTP request methods](http://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods).\n* Often additional options and parameters must be passed to specify how the command should take place.\n\nFrom that then, it follows that each web API URL (command) is composed of three parts:\n\n1. an HTTP method - the verb\n2. a URL path - the resource\n3. and any additional arguments or parameters the API command may need\n\n<br />\n-----------------------------------------------------------\n\nA Sample Use\n============\n\nWe'll slowly build a fairly complex script using the API to:\n\n* Create a history\n* Upload a file from our local system to the server and place it in that history (data/myIlluminaRun.solexa.fastq -\n  also available from [this published history on Galaxy Main](https://main.g2.bx.psu.edu/u/aun1/h/quickie-14))\n* Run a workflow on that file <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/Galaxy-Workflow-Joined_Solexa_QC.ga> (already available in your instance)\n* Get some information about the workflow\n* Rename the file and annotate it\n* Copy the file to a library\n\nThe final script should not be considered a 'good' script, however, because a good script would take into account:\n\n* Complications: typically in a robust script/application, a significant amount of code will go towards handling\n  [corner-cases](http://en.wikipedia.org/wiki/Corner_case). For the sake of making the learning examples clear and\n  easy to follow, we won't cover very many of these situations.\n* Error handling: errors can and will occur during development. Again in this case, we'll only make limited use\n  of python's error handling capability and a real script would have more and better handling. **We want everyone to be\n  able to follow along, however, so if you do see an error - please, let us know and we can try and get you past it.**\n* Windows compatibility: these scripts aren't tested with Windows (but they should work with some minor tweaking).\n\nThere are 10 steps total to building the script. Each step (step\\_1.py, step\\_2.py, ...) has its own progressively longer\nversion of the final script, adding new sections and new functionality.\n\nAs each step introduces new functionality in the API, they will themselves include new, short modules that center\naround some resource and the API methods that can be applied to them. For example, when writing step\\_2.py, we'll create\nseveral history scripts (histories\\_1.py, histories\\_2.py) that each add more to what we can do with histories. step\\_2.py\nwill then `import` these smaller modules to combine them into our final script.\n\nAlso note: that almost every 'resource' script can function on its own, providing some functionality (such as querying\nor creating).\n\n<br />\n-----------------------------------------------------------\n\nThe Set Up\n==========\n\n* If you haven't already, download the [virtual machine (VM) image](http://wiki.galaxyproject.org/Events/GCC2013/TrainingDay/VMs) for this workshop.\n* If you haven't already, start the VM\n* fire up Etherpad (https://etherpad.mozilla.org/hCF6QedfLD)\n* fire up the terminal\n* directory structure for the workshop\n* Galaxy start up - mention the use of a local installation (or a development installation) for exploring the API\n* open gedit (or other) in order to load the scripts in each step\n\nThe API Key\n-----------\n\n* load the galaxy home page, login, and go to the top menu 'User' â†’ 'API Keys'\n* Let's look for the api key (don't change it or press the 'Generate' button)\n\nYour **API key** is unique identifier that is sent to the API to link you with your Galaxy account and checks your\naccess to data. It is effectively your username and password. **Treat it with the same caution and care you would your\npassword**.\n\nFor the purposes of simplifying some of our scripts, we're going to add our API key as an environment variable in our\nterminal session.\n\n```wiki solid/red\n**Keeping any form of unencrypted authentication can be dangerous and that includes your API key**.\nIf you use the following technique, place the `source`ed environment variable file in a secure location on your\nlocal filesystem.\n```\n\nLet's load the API key into our terminal session:\n\n```bash\nsource apiworkshop-setup.sh\n```\n\nAnd check what environment variables it's added. First, the API key:\n\n```bash\necho $GALAXY_API_KEY\n```\n\nthen the **base url** of where our Galaxy installation is served:\n\n```bash\necho $GALAXY_BASE_URL\n```\n\nThese will both be needed in every call to the API.\n\nStructure\n---------\n\nWe'll be using python and [scripts/api/common.py](https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/common.py) as a layer on top of urllib2.\n<https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/setup.py> will load our key and base URL for every API call.\n\nMuch of the functionality of the resource scripts (such as users\\_1.py, histories\\_1.py, etc.) is available already\nin BioBlend or Blend4j - we won't use them here so we can get a closer look on the internals and conventions of the API.\n\n<br />\n-----------------------------------------------------------\n\nThe Steps\n=========\n\n<br />\n\n1\\. Checking your key with the user API\n---------------------------------------\n\nWe'll start with the scripts: <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/step_1.py> and <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/users_1.py>.\n\nThese are simple scripts that only make a connection, call GET api/users, and return information about us - the current\nuser.\n\nOf course, normally we already know this information but this allows us to **test whether our server, url, and key are\nworking properly**.\n\nTaking a look inside users\\_1.py we can see we're simply calling a function in our `main` clause: get_users. It takes\nour key (since it's our form of authentication here) and the root URL, builds on that URL with the api/ + resource and\nthen calls `common.get` with those parameters (which does some of the heavy/tedious lifting here).\n\n```wiki solid\nAll API method calls will follow this pattern: build a URL from a base URL and a resource then pass it to an HTTP method\nin `common.py` with the key authentication.\n```\n\n```bash\n./users_1.py\n```\n\nHere's the data the users\\_1.py script should return (if all went well):\n\n```bash\n[ { u'email': u'apiworkshop@example.com',\n    u'id': u'1cd8e2f6b131e891',\n    u'model_class': u'User',\n    u'quota_percent': None,\n    u'url': u'/api/users/1cd8e2f6b131e891'}]\n```\n\n(Note: your ID may be different)\n\nNote the syntax of what's returned: a list with one element, the element is in python dictionary form, and the strings\nare unicode strings. Most resource data returned from the API will be in dictionary form (after having been converted\nfrom JSON objects in common.py). It's a list because we used the index API method (we'll cover that in step 2) and\nthere's only one element because we're only allowed to access our own user data. Unicode strings will be a common sight\nduring the workshop for both dictionary keys and values. They're a special string for encoding many human languages and\nwriting systems but for all intents and purposes here they can be thought of as normal strings.\n\nNow - try the first iteration of our main script, step\\_1.py.\n\n```bash\n./step_1.py\n```\n\nYou should see the same data returned as users\\_1.py - we're not doing much yet.\n\n<br />\n\n2\\. Looking at your histories with the history API\n--------------------------------------------------\n\nHere we'll learn how to query all our histories and query a specific history.\n\nScripts: <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/step_2.py>, <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/histories_1.py>, <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/histories_2.py>\n\n<br />\n\n### histories\\_1.py\n\nhistories\\_1.py allows us to get (summary) data on all our histories using the GET method and the index API method.\n\nCalling histories\\_1.py from the command line should show us a single, 'Unnamed history' complete with Id, name, and\nother attributes. Note that, even though there is only one history, the form of the data returned is a list:\n\n```bash\n[ { u'deleted': False,\n    u'id': u'c24141d7e4e77705',\n    u'model_class': u'History',\n    u'name': u'Unnamed History',\n    u'published': False,\n    u'tags': [],\n    u'url': u'/api/histories/c24141d7e4e77705'}]\n```\n\nhistories\\_1.py is an **index** API method. These are generally used to return lists of often summary information about\na group of resources (e.g. all my histories, or the results of a search query such as 'all histories whose names begin\nwith X').\n\n<br />\n\n### histories\\_2.py\n\nhistories\\_2.py allows us to get more detailed info about a *specific* history also using the GET command.\n\nYou must call histories\\_2.py from the command line and give it a single Id as a parameter - in this case, copy the\nid attribute from the history returned from histories\\_1.py (without quotes) and paste it after the histories\\_2.py script\nname:\n\n```bash\n./histories_2.py 1cd8e2f6b131e891\n```\n\n(Again: the ID may be different - make sure you paste the one from histories\\_1.py)\n\nIn histories\\_2.get_history, we're building the url and including an ID in it to get data about a specific history.\nMany resources are stored in the database with an ID and adding them to the url allows us to specify which resource\nwe want to use. Even though were using the same HTTP method/verb as index here (GET), because we've added the id to the\npath, the API now recognizes we want to do a **show**: get specific information on *one* history. The way you build\na URL (and what/how you add extra parameters) can make each of the four basic HTTP methods do very different things.\n\nYou should get a single python dictionary containing quite a bit of information about your current history. Although\nyou have no datasets in it yet, notice the attributes which would contain the counts and ids of datasets keyed by\ntheir potential states.\n\nNote that histories\\_2.py is the same file as histories\\_1.py with a new function added in. The resource scripts will\nall follow this pattern. The 'last' resource script will always contain the functionality of the ones 'before' it, just\nas each step_N will add new functionality to the overall, final script. (Warning: the effects of calling the resource\nscripts on the command line will change however from number to number - it will often be a demonstration of the most\nrecently added functionality.)\n\nAgain:\n\n* **index**: histories\\_1.py: GET api/histories â†’ a list of histories for the current user\n* **show**: histories\\_2.py: GET api/histories/\\[a history id] â†’ a single, specific history\n\nMany resources in the Galaxy API will have index and show methods for reading and retrieving information.\n\n<br />\n## Errors!\n\nNow that we have something we can pass an argument to (and therefore break), let's break it! Try entering this:\n\n```bash\nhistories_2.py bler\n```\n\nYou should see a stack trace. You should already be familiar with a stack trace but, if you're not, note the two\nmost important pieces of info it provides: an error string (`urllib2.HTTPError: HTTP Error 500: Internal Server Error`)\nand a location in a script file. There is difference here from a normal error in a local script: although we made a\nmistake the stack trace location won't help us much because the relevant code is on the server. We can only use the\nerror string to let us know what happened.\n\nIn many places in the Galaxy API 'HTTP' errors and exceptions will be thrown, both with (hopefully) descriptive\nmessages and also with an [HTTP status code](http://en.wikipedia.org/wiki/List_of_HTTP_status_codes). Generally when\nyou make an error using the API, Galaxy will (or should) return something in the 400's (meaning user error). In this\ncase, even though it was clearly our fault, Galaxy returned a 500 (Internal Server Error) meaning something went wrong\non it's side.\n\n(We're striving to make this more consistent - but when using the API you should be aware of this pitfall)\n\n<br />\n## 3. Creating a history with the history API\n\nHere we'll learn how to create a new history which will (automatically) become our 'working' history.\n\nScripts: <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/step_3.py>, <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/histories_3.py>\n\n<br />\n### histories_3.py\n\nWe've added the function `create_history` in this version of `histories`. This will build a URL to `api/histories`\nand call that URL with the HTTP method POST (common.post). Note that we don't need to send an ID as this is a new\nhistory and Galaxy will assign the ID itself (the new ID will be returned to us so we can use to further manipulate\nthe history later). This is a common pattern with **create**.\n\nNote also that `create_history` allows us to immediately name the history by passing in a `name` argument. The name is\npassed to `common.post` bundled in a dictionary. This is also a common pattern with POST (and PUT) and, although these\nadditional arguments are often optional in practice, there are cases were they're required. It's generally best to\ncheck the [API documentation](http://galaxy-dist.readthedocs.org/en/latest/lib/galaxy.webapps.galaxy.api.html) or\nthe code to find out.\n\nhistories\\_3.py is also set up to create a new history from the command line now but since we're also doing that in\nstep\\_3.py, we'll just call that:\n\n```bash\n./step_3.py\n```\n\nThe output:\n\n```bash\ncreated history! Step 3\nnew history:\n{ u'annotation': u*,\n  u'contents_url': u'/api/histories/ee3742ed6d8b4a0c/contents',\n  u'deleted': False,\n  u'genome_build': None,\n  u'id': u'ee3742ed6d8b4a0c',\n  u'model_class': u'History',\n  u'name': u'Step 3',\n  ...\n```\n\n(The output has been truncated here with '...')\n\nAccording to the script we created a new history with a name of 'Step 3'.\n\nWe can double check this a couple of different ways - we could refresh Galaxy in our browser and this history should\nbe our current, 'working' history. Or we could call either histories\\_1.py (which will have a list of our histories that\nshould contain the new one) or step\\_2.py (which will give details about our current history).\n\nSince each step_N.py script will build on the previous, we'll be creating a new history for each step and each new\nhistory will be named 'Step N'. This will allow us to 'start fresh' with each step and not worry about 'polluting' any\none history. You can even call ./step\\_3.py (or any of the step scripts) multiple times without fear of ruining any\nfollowing steps.\n\n<br />\n## 4. Checking the contents of a history with the history contents API\n\nHistories can be thought of as containers for the datasets we produce over time. The python class for those datasets\n(that are associated with a history and a user) is HistoryDatasetAssociation (HDA).\n\nHere we'll learn how to query the datasets *within* a history using a **contents** style API URL. This is\nvery similar in functionality to the right-hand history panel in the Galaxy web UI - in fact that panel uses a version\nof this API call to build its HTML.\n\nScripts: <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/step_4.py>, <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/hdas_1.py>\n\nhdas\\_1.py will do an `index` of the hdas if you call it from the command line. Examine hdas\\_1.get_hdas. In some ways\nit's very similar to histories.get_history (the `show`) but we've added `/contents` to the URL path.\n\nIf we want to get the HDAs from a history we need to tell the API two things: 1) the specific history we want the HDAs\nfrom and 2) that we're interested in the 'contents' and not the information on the history itself (which we would get\nfrom `show`). This is a common pattern for the API when dealing with **containers**, we need to specify the containers\nand tell the API to do something with the *contents*.\n\n./step\\_4.py does everything that steps 1 to 3 do and then queries the HDAs in the new history:\n\n```bash\n./step_4.py\n```\n\nThe output:\n\n```bash\ncreated history! Step 4\nHDAs:\n[]\n```\n\nNote the empty list for the HDAs. Since this is an `index` of the HDAs we know it will return a list, but the emptiness\nis a bit anti-climatic. This is a new history, however, so it is the correct return value. If it had HDAs to return,\nthat would have been a pretty big bug!\n\n<br />\n## 5. Uploading a file to a new HDA with the history contents API\n\nHere we'll start moving a bit faster: we'll upload a file from our 'local' machine into a new history and get some\ndetails on the HDA that Galaxy creates for it.\n\nScripts: <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/step_5.py>, <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/tools_1.py>, <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/hdas_2.py>\n\nThe file will be `myIlluminaRun.solexa.fastq` and contain some solexa fastq reads. As an aside: this file is from the\nhistory uploaded by Anton Nekrutenko at Galaxy Main server's published histories:\nhttps://main.g2.bx.psu.edu/u/aun1/h/quickie-14.\nIt's the initial data used in the screencast quickie: http://screencast.g2.bx.psu.edu/quickie\\_13\\_fastq_basic/flow.html\n\nThe tools\\_1.py contains one function: upload_hda. The Galaxy developers would like to have every tool be available to\nrun through the API but this is still a work in progress. In this case, we use another module/script\n<https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/upload_to_history.py> to remove some of the complexity and it itself uses the `requests` module to handle\nmoving the file into the POST data. Be sure to check out those modules for more details on what's involved.\n\nThe takeaway here is that, to run a tool we combine four things:\n\n1. the POST HTTP method (or `create` again)\n2. the resource URL `api/tools`\n3. the ID of the tool to use: `upload1`\n4. and any parameters the tool needs: the file, a name, some extra options and directives\n\n```bash\n./step_5.py\n```\n\nThe output:\n\n```bash\ncreated history! Step 5\nuploaded hda! myIlluminaRun.solexa.fastq\nHDAs now:\n[ { u'id': u'9d052533bb57bd3e',\n    u'name': u'myIlluminaRun.solexa.fastq',\n    u'type': u'file',\n    u'url': u'/api/histories/9752b387803d3e1e/contents/9d052533bb57bd3e'}]\nUploaded:\n{ u'accessible': True,\n  u'api_type': u'file',\n  u'data_type': u'auto',\n  u'deleted': False,\n  u'display_apps': [],\n  u'display_types': [],\n  u'download_url': u'/api/histories/9752b387803d3e1e/contents/9d052533bb57bd3e/display',\n  u'file_ext': u'auto',\n  u'file_size': 0,\n  u'genome_build': u'?',\n  u'hda_ldda': u'hda',\n  u'hid': 1,\n  u'history_id': u'9752b387803d3e1e',\n  u'id': u'9d052533bb57bd3e',\n  ...\n```\n\nThere's our uploaded file in Galaxy.\n\n<br />\n### Asynchronous\n\nNote the `state` attribute of the uploaded file:\n\n```bash\nu'state': u'queued',\n```\n\nThis means that the job that uploads the HDA isn't actually finished when our script is. In fact, since many things\nhappen asynchronously (not always one-after-another) - Galaxy's infrastructure, the API, and calls to it are designed\nto return data and continue even if a particular function or command is not 'finished' yet. Essentially, what\n`upload_hda` does is tell Galaxy to bring in the file, return some data immediately, and then move on to the next\ncommand in the script.\n\nThis is an advantage in many situations and a disadvantage in others.\n\nAn advantage is that **you don't have to wait for Galaxy's jobs to finish before sending more commands**: you could\ncall the `upload_hda` 100 times for 100 files within a minute and all those jobs will queue while you go get a beer.\nGalaxy will create those 100 jobs in the queue while you do other things.\n\nIf you have to use the data from an asynchronous operation like running a tool or a workflow, it becomes (a slight)\ndisadvantage and you **need to wait for something to finish**. Handling this properly is something we'll cover in\nstep 6.\n\n<br />\n## 6. Uploading a file and waiting until the API says it's 'ok'\n\nHere we'll handle the asynchronous nature of Galaxy tool running and jobs in order to be sure the data is uploaded\nand ready before we start using it.\n\nScripts: <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/step_6.py>\n\nMost of our changes will be 'local' this time in the step\\_6.py version of our final script. We'll use a while loop\nand the `state` information from the HDA `show` api to **wait** for the upload to finish.\n\nThe psuedo code for this loop might look like this:\n\n```\nGet the state of the HDA from the API\nWhile the state isn't 'ok':\n  Wait for four seconds\n  Check the state again using the API\n```\n\nWe'll also output some of that information while our script is running. This is always a good idea especially during\ndevelopment so you know what's going on.\n\n```bash\n./step_6.py\n```\n\nThe output:\n\n```bash\ncreated history! Step 6\nuploaded hda! myIlluminaRun.solexa.fastq\n\t uploaded_hda_state: queued\n\t (waiting 4 seconds...)\nUploaded:\n{ u'accessible': True,\n  u'api_type': u'file',\n  u'data_type': u'fastq',\n  ...\n```\n\n(Note: you may see more waiting when you run the script)\n\nNow we know that when the loop completes in our script the dataset is finished uploading and any operations we do after\nthe loop will have access to the dataset's data.\n\n<br />\n## 7. Running a workflow on an uploaded HDA using the workflows API\n\nNow we'll run a workflow on our uploaded file and again wait for all of it to finish. We'll be moving faster here.\n\nScripts: <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/step_7.py>, <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/workflows_1.py>\n\n<br />\n### Workflows\n\nWorkflows are complicated structures. For this workshop, the needed workflow here is already loaded and available to\nthe workshop user (but it is also available at TODO). All we need to do is 'invoke' it from the api on the uploaded HDA.\nThis is another example of `POST + resource URL + parameters` to `create`. It's also an example of **composing**\ncomplex scripts - this time using the server side instead of the client.\n\nNote: there are some rough ways to change the parameters used in workflows dynamically through the API. Be sure to\ncheck out `scripts/api/workflow_execute` and `scripts/api/workflow_execute_parameters` for examples.\n\n```bash\n./step_7.py\n```\n\nThe output:\n\n```bash\ncreated history! Step 7\nuploaded hda! myIlluminaRun.solexa.fastq\n     uploaded_hda_state: queued\n     (waiting 4 seconds...)\nfound workflow! Joined Solexa QC\nrunning Joined Solexa QC workflow...\nworkflow started!\ngroomed.fastq\n     state: running\n     (waiting 4 seconds...)\n     ok\nfiltered at > 17\n     state: running\n     (waiting 4 seconds...)\n     state: running\n     (waiting 4 seconds...)\n     state: running\n     (waiting 4 seconds...)\n     state: running\n     (waiting 4 seconds...)\n     ok\nstatistics\n     state: running\n     (waiting 4 seconds...)\n     ok\nforward reads\n     ok\nreverse reads\n     ok\nstatistics plot\n     ok\nworkflow complete!\nOutput:\n{ u'history': u'df7a1f0c02a5b08e',\n  u'outputs': [ u'f597429621d6eb2b',\n                u'1cd8e2f6b131e891',\n                u'ebfb8f50c6abde6d',\n                u'33b43b4e7093c91f',\n                u'a799d38679e985db',\n                u'5969b1f7201f12ae']}\n```\n\nNote the final output: these are the HDAs created by the workflow. (We can also list them using hdas\\_2.py.)\n\n<br />\n## 8. Checking the data of an HDA using the datasets API\n\nNow we'll introduce the possibility of a **conditional** in our API script: specifically, we'll *look inside the\ndataset data* of one of the HDAs created by our workflow. This will give us an opportunity to do something different\ndepending on what we find.\n\nScripts: <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/step_8.py>, <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/datasets_3.py>\n\n<br />\n### The datasets API\n\nIt's all fine and good to operate on the information from resources, but if we consider the actual 'currency' of\nGalaxy to be the *data inside datasets* we'll need to get at that information as well.\n\nNote that we've skipped talking about `datasets_1` and `datasets_2`. These would be the `index` and `show` methods\nfor the datasets resource. In many ways, the datasets resource is a simplification of the HDA resource (and it's sibling\nthe LibraryDatasetDatasetAssociation or LDDA resource - which we'll cover a bit). Datasets is an interface to both HDAs\nand LDDAs that often (effectively) 'strip' or ignore the information about the datasets' containers.\n\nLet's try datasets\\_1.py, tho, to see something important:\n\n```bash\n./datasets_1.py\n```\n\nThe output:\n\n```bash\nTraceback (most recent call last):\n...\nurllib2.HTTPError: HTTP Error 501: Not Implemented\n```\n\n(Again, the '...' was added to simplify the output)\n\n`HTTP Error 501: Not Implemented` is a server error (with status code 501) meaning this functionality hasn't been\nadded yet (and may never be).\n\nIn this case, you're seeing it because the developers don't have an implementation of `index` for datasets (what might\nthat be? A list of all datasets for the current user? All published datasets? An interface to a dataset search?) In any\nevent, that functionality isn't available throught the API at this time. Keep this error in mind when exploring the API.\n\n`datasets_2.py` in many respects mimics the functionality of `hdas_2.show` and returns simple data and metadata for\nthe dataset.\n\nHowever, the same API method from `datasets_2` (\\`GET + resource_url + ID), when passed some optional parameters\nallows us to **get data from within the dataset**. This is done through the Galaxy construct of data providers.\n\n<br />\n### Data providers\n\nData providers allow us to get specific data from within a dataset's file contents. In this case\n(`datasets_3.get_column_data`), we're using the ColumnDataProvider which provides columns from the Tabular dataset\n`statistics`.\n\nData provider are also a work in progress and currently only used in visualizations, but work is underway to make\nthem easier to use and more powerful in general.\n\nSee the `datasets_3.get_dataset_column` for an example of how to use the datasets API to get raw data from a file.\n\nLet's now run step 8:\n\n```bash\n./step_8.py\n```\n\nThe output:\n\n```bash\nuploaded hda! myIlluminaRun.solexa.fastq\n     uploaded_hda_state: queued\n     (waiting 4 seconds...)\nfound workflow! Joined Solexa QC\nrunning Joined Solexa QC workflow...\nworkflow started!\n...\nworkflow complete!\nget_dataset_column, full_url: http://localhost:8081/api/datasets/3f5830403180d620?key=a63a28c5d5575a8ca78e97e01a73f901&data_type=raw_data&provider=column&columns=[5]\nmean: 38.8721137988 median: 19.776113249\n```\n\nSo now we've gotten some quality information about our workflow run and fastq reads. As `step_8.py` mentions, we\ncould use that information to *conditionally* run a seperate script, provide some warning/email to the user, etc.\n\n(Also note the print out of the get_dataset_column full url: see `step_8.py` and `datasets_3.py` for an explanation of\nwhy but, in the simplest terms, its an illustration of how some API calls transmit extra parameters.)\n\n<br />\n## 9. Renaming and annotating an HDA using the history contents API\n\nWe'll now introduce a third HTTP method: PUT (DELETE won't be covered in this workshop but know that it's available\nfor many resources) and use it to: **update** (change) the name of our final, desired fastq HDAs and annotate them\nwith the quality information from step 8.\n\nScripts: <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/step_9.py>, <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/hdas_3.py>\n\nPUT/`update` methods are meant to alter existing resources - much like the edit attributes page of the Galaxy UI. Their\nURLs are constructed very similarly to any GET method where we specify resources, containers, and contained objects\nby id. We pass the data we want to change in the form of a dictionary and that gets packaged into the HTTP request\nmuch like POST data does.\n\nhdas\\_3.py adds the function `update_hda` which allows us to change the state and information contained in the HDA\n(but *not* the file contents). We can change the name, annotation, `misc_info` (the small amount of text that\ndescribes what the HDA's file contains), the genome build (or `dbkey`) and whether this HDA is visible and/or deleted.\nSetting `deleted` to true does not mean the file and HDA are purged from the filesystem and database only they are\n'marked as deleted' and any admin scripts that run later can purge them. The closest analogy is your desktop OS's\ntrash bin - in this way Galaxy allows for a reversible step before the actual file/database removal.\n\nMany `update`/PUT API methods will only allow the direct change of a limited set of attributes. It's generally best\nto check the documentation or source to find out what has been enabled in an `update` method.\n\nThe `update` method of the HDA API is designed to accept the same dictionary that's returned from the HDA `show`\nmethod. This allows us to capture the dictionary from that method, change what we need, and send the same dictionary to\nthe `update` method. It also allows a partial dictionary with just the keys and new values we want to change.\n\nIn either case, the `update` method returns a dictionary of what has successfully been changed.\n\nNote: unfortunately, at the time this was written the HDA annotations were not passed back in the HDA `show` method.\nAlthough we do change the annotations of the read files in step\\_9.py, you'll need to check for the changed annotations\nin your browser - by navigating to the most recent 'Step 9' history and opening the annotations of the forward and\nreverse reads.\n\nStep 9 will, however, show us the changed names of the HDAs:\n\n```bash\n./step_9.py\n```\n\nThe output:\n\n```bash\ncreated history! Step 9\nuploaded hda! myIlluminaRun.solexa.fastq\n     uploaded_hda_state: queued\n     (waiting 4 seconds...)\nfound workflow! Joined Solexa QC\nrunning Joined Solexa QC workflow...\nworkflow started!\n...\nworkflow complete!\nget_dataset_column, full_url: ...\nmean: 38.8721137988 median: 19.776113249\nfound fwd/rev reads: forward reads reverse reads\nForward: myIlluminaRun.fwd.fastqsanger\nReverse: myIlluminaRun.rev.fastqsanger\n```\n\n\n\n<br />\n## 10. Copying an HDA to a library using the library contents API\n\nFor our final step, we'll take our quality controlled and annotated read files and copy them into a shared Library\nfor everyone in our lab to use.\n\nScripts: <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/step_10.py>, <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/libraries_1.py>, <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/lddas_1.py>\n\nThe API is a great way for administrators and 'power-users' (that's you now) to **create and manage shared resources**\nquickly and easily. This makes life easier for people at your lab that are more comfortable with the UI (or those that\nsimply 'explore' the data more there) by creating accessible, shared data and makes your life easier by automating all\nor parts of the frequent bioinformatic requests you may get.\n\n<br />\n### libraries_1.py and lddas_1.py\n\nLibraries are unique containers (when compared to histories) in that they can contain *other containers*. These\nnested containers are folders and add a layer of complexity to our last task.\n\nFor the purposes of simplifying this workshop, we'll only copy our reads to the root folder of an existing library, but\nyou should be aware that the Library/LDDA API is capable of creating and modifying both libraries and any amount of\nfolders within them. Libraries (and their contents) can also be imported or exported through the API, effectively\nmaking it a good candidate of data communication or migration **between Galaxy instances**.\n\nStep 9 will, however, show us the changed names of the HDAs:\n\n```bash\n./step_10.py\n```\n\nThe output:\n\n```bash\ncreated history! Step 10\nuploaded hda! myIlluminaRun.solexa.fastq\n     uploaded_hda_state: queued\n     (waiting 4 seconds...)\nfound workflow! Joined Solexa QC\nrunning Joined Solexa QC workflow...\nworkflow started!\n...\nworkflow complete!\nget_dataset_column, full_url: http://localhost:8081/api/datasets/ba751ee0539fff04?key=a63a28c5d5575a8ca78e97e01a73f901&data_type=raw_data&provider=column&columns=[5]\nmean: 38.8721137988 median: 19.776113249\nfound fwd/rev reads: forward reads reverse reads\nchanged read HDA names and annotations: myIlluminaRun.fwd.fastqsanger myIlluminaRun.rev.fastqsanger\nfound reads library: Reads Library\nfound root folder: /\ncopying HDAs to library: Reads Library\n     forward read: myIlluminaRun.fwd.fastqsanger\n     reverse read: myIlluminaRun.rev.fastqsanger\n...\nForward reads in library:\n{ u'data_type': u'fastqsanger',\n  u'deleted': False,\n  u'file_name': u'/home/gcc2013/Desktop/Training_Day_Workshops/API/galaxy-central/database/files/000/dataset_26.dat',\n  u'file_size': 1093696,\n  u'genome_build': u'?',\n  u'hda_ldda': u'ldda',\n  ...\nReverse reads in library:\n{ u'data_type': u'fastqsanger',\n  u'deleted': False,\n  u'file_name': u'/home/gcc2013/Desktop/Training_Day_Workshops/API/galaxy-central/database/files/000/dataset_27.dat',\n  u'file_size': 1093696,\n  u'genome_build': u'?',\n  u'hda_ldda': u'ldda',\n  ...\n```\n\nAnd ... we're done!\n\n<br />\n-----------------------------------------------------------\n# In the Future\n\nWe are always adding new functionality to the API and progressively ensuring that everything that can be done in the\nUI can be done in the API - because of that, please, consider the API a 'work in progress' and check the\n[Galaxy Development Briefs](/docs/) for new features or fixes.\n\n<br />\n### Versioning of the API\n\nAs things change in Galaxy and we make improvements to the API, there will be times when the data that's passed to\nan API method, the data that's returned, or the fundamental effects of an API method *should* change.\n\nThe fact that people have and will continue to write scripts that depend on the API functioning in a particular way\ncomplicates this issue and the developers know that a certain amount of **backwards compatibility** should be\nmaintained.\n\nAlthough the API should still be considered a 'work in progress', there are plans to have **two versions** of the\nAPI. The first, would be the 'stable' api and the second: the 'development' version. The development version may not\nalways behave in the same manner as the stable (hopefully it will be an improvement) and shouldn't be relied upon\nas a source for data or control. The 'stable' version is meant to be used and consumed and should provide the most\nstable, secure, and robust system for scripting the API.\n\nAs the development version nears a point of completion and reliability itself, the previous stable version will\nbe replaced with it and a new development version will be started.\n\n<br />\n### Tool Running\n\nWe know that running tools via the API would be a big win for everyone - so it's a priority on our list. Before that\nhappens some amount of core changes need to take place to make this as easy and flexible to use as possible.\n\n<br />\n-----------------------------------------------------------\n\nMore Resources and Thanks\n=========================\n\n* a tarball of all the scripts used is here: <https://depot.galaxyproject.org/hub/attachments/events/gcc2013/training-day/api/all-scripts.tar.gz>\n\n* [ReadTheDocs.org](http://galaxy-dist.readthedocs.org/en/latest/lib/galaxy.webapps.galaxy.api.html).\n\n* There are good examples in the `scripts/api` directory of a Galaxy installation.\n\n* BioBlend and Blend4j (links are at the top of this page) can make your API scripting easier.\n\n* Thanks for coming!\n\n<slot name=\"/events/gcc2013/footer\" />\n"}},"context":{}}