{"hash":"14e186fd360947c1255e65c919a062692f477385","data":{"article":{"id":"9d1208a98bfd5d3c3ae7c527515ae17a","title":"","tease":"","skip_title_render":null,"image":"","images":{},"contact":"","inserts":[{"name":"/cloudman/header","content":"<div class='center'><img src=\"/images/galaxy-logos/cloudman-logo.jpg\" alt=\"CloudMan Logo\" width=\"50%\" /></div>\n<br />\n"},{"name":"/cloudman/linkbox-horizontal","content":"<div style=\"margin-bottom: 15px; padding: 0 5px; background-color: #DFE5F9;\">\n<p><a href=\"/cloudman/\">CloudMan home</a> |\n<a href=\"/cloudman/getting-started\">Getting Started</a> |\n<a href=\"/cloudman/cluster-types\">Cluster Types</a> |\n<a href=\"/cloudman/capacity-planning\">Capacity Planning</a> |\n<a href=\"/cloudman/services\">Application Services</a> |\n<a href=\"/cloudman/sharing\">Cluster Sharing</a> |\n<a href=\"/cloudman/userdata\">Instance User Data</a> |\n<a href=\"/cloudman/customizing\">Customizing</a> |\n<a href=\"/cloudman/building\">Building on a private cloud</a> |\n<a href=\"/cloudman/troubleshooting\">Troubleshooting</a> |\n<a href=\"/cloudman/faq\">FAQ</a></p>\n</div>\n"}],"date":null,"content":"<slot name=\"/cloudman/header\" />\n\n<slot name=\"/cloudman/linkbox-horizontal\" />\n\nThis page will provide details on how HTCondor has been integrated into CloudMan. Examples on how to use HTCondor through CloudMan will be provided for the enduser.\n\nInstallation Configuration\n==========================\n\nHTCondor has been pre-installed on linux images using through CloudMan. The deployment detail is as follow:\n\n* bin                          /usr/bin\n* etc                          /etc/condor/\n* etc/examples                 /usr/share/doc/condor/etc/examples\n* examples                     /usr/share/doc/condor/examples\n* include                      /usr/include/condor\n* lib                          /usr/lib/condor\n* libexec                      /usr/lib/condor/libexec\n* local/condor_config.local    /etc/condor/\n* condor_config                /etc/condor/condor_config\n* local/execute                /var/lib/condor/execute\n* local/spool                  /var/lib/condor/spool\n* man                          /usr/share/man\n* sbin                         /usr/sbin\n* sql                          /usr/share/condor/sql\n* src                          /usr/src\n* INIT                         /etc/init.d/condor\n* PID                          /var/run/condor\n* LOGS                         /var/log/condor\n* LOCK                         /var/lock/condor\n\nPlease note that the configuration is taken place only in **condor_config** global file. The online configuration is based on the functionality of the HTCondor where only the last configuration value is used. Therefore, for finding any specific configuration in /etc/condor/condor_configuration please find the last key in the file.\n\nHTCondor requires the ping port \"ICMP\" and the range specified through **HIGHPORT** and **LOWPORT** in the condor_configuration to be open. To do so please refer to the firewall options.\n\nHTCondor Flocking\n=================\n\nHTCondor has been used to achieve federated computing resource manager over the remote computing resources. In this scenario HTCondor will receive the remote computing public IP or DNS and try to use remote resources by claiming the resource from remote HTCondor pool. This has happened by setting the **FLOCK_TO** variable in the condor_config file to be the remote IP or DNS. It worth noting that the remote Condor should have provided enough acces to the CloudMan HTCondor by setting **FLOCK_FROM** and **ALLOW_WRITE** variable in their condor_config file (This setting requires HTCondor to be restarted. To restart HTCondor manually in CloudMan use */etc/init.d/condor restart* command).\n\nBig Pool\n========\n\nTo be able to utilize local resource through HTCondor any worker node's pool added to the system through CloudMan console will be joint to the master's pool to create a big shared pool. This happen by setting the workers' \\*\\*CONDOR_HOST \\*\\* variable to point to the master IP and set the  **DAEMON_LIST** to only run **MASTER, STARTD and SCHEDD** processes.\n\nExample\n=======\n\nFor an example please follow the following procedure:\n\n1- $mkdir test<br />\n2- $cd test<br />\n3- $cat > python_random_lines.sh<br />\n\\#! /bin/sh<br />\necho \"I'm process id $$ on\" `hostname`<br />\npython random_lines_two_pass.py \"dataset\\_$2.dat\" \"output\\_$2.dat\" \"3\"<br />\ndate<br />\nexit 42<br />\n*ctrl+D*<br />\n4- $cat > job.submit <br />\nexecutable=python_random_lines.sh<br />\nuniverse=vanilla<br />\narguments=Example.$(Cluster).$(Process) \\[file_number] <br />\ntransfer_input_files = dataset_\\[file_number].dat<br />\ntransfer_output_files = output_\\[file_number].dat<br />\noutput=results.output.$(Process).$(Cluster)<br />\nerror=results.error.$(Process).$(Cluster)<br />\nlog=results.log<br />\nnotification=never<br />\nshould_transfer_files=YES<br />\nwhen_to_transfer_output = ON_EXIT<br />\nqueue<br />\n*Ctrl+D*<br />\n**Note that for running the above script you need to have random_lines_two_pass.py and the dataset available**<br />\n**The job.submit should be run multiple times \\[once for each input file]**<br />\n5- chmod a+x python_random_lines.sh<br />\n6- condor_submit job.submit<br />\n**condor_submit should be run multiple times \\[once for each input]**\n"}},"context":{}}